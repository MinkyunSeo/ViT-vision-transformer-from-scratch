{"cells":[{"attachments":{},"cell_type":"markdown","id":"mDOdbok4zVE4","metadata":{"id":"mDOdbok4zVE4"},"source":["# Colab Setup"]},{"cell_type":"code","execution_count":null,"id":"-NvjxSCiMfMq","metadata":{"id":"-NvjxSCiMfMq"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"id":"oS5qDEF5MftN","metadata":{"id":"oS5qDEF5MftN"},"outputs":[],"source":["\"\"\"\n","Change directory to where this file is located\n","\"\"\"\n","%cd 'COPY&PASTE FILE DIRECTORY HERE'"]},{"attachments":{},"cell_type":"markdown","id":"giBbJLW-zhS-","metadata":{"id":"giBbJLW-zhS-"},"source":["# Import Modules"]},{"cell_type":"code","execution_count":null,"id":"KjGAWailzgP1","metadata":{"id":"KjGAWailzgP1"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","from torchvision import transforms, datasets\n","from tqdm.auto import tqdm"]},{"attachments":{},"cell_type":"markdown","id":"2eeea83e","metadata":{"id":"2eeea83e"},"source":["# ViT Model  \n","\n","An Image is worth 16*16 words: Transformers for image recognition at scale  \n","https://arxiv.org/pdf/2010.11929.pdf\n","\n","![Image](architecture.png)"]},{"cell_type":"code","execution_count":null,"id":"ZlLE3KG6VN6h","metadata":{"id":"ZlLE3KG6VN6h"},"outputs":[],"source":["class Patchification(nn.Module):\n","  \"\"\"\n","  Divide image into patches \n","  Input shape: [batch, channel, height, width]\n","  Return: [batch, number_of_patches, embedding_dimension]\n","  \"\"\"\n","  def __init__(self, in_channels, patch_size, embedding_dim):\n","    super().__init__()\n","    \n","    # embedding_dim == out_channel of convolution.\n","    self.conv = nn.Conv2d(in_channels, embedding_dim, kernel_size=patch_size, stride=patch_size)\n","\n","  def forward(self, x):\n","\n","    # input x shape: [batch, channel, height, width]\n","    x = self.conv(x)          \n","    B, E, H, W = x.shape                # x shape [batch, embedding_dim, height/patch_size, width/patch_size]\n","    x = x.permute(0, 2, 3, 1)           # x shape [batch, height/patch_size, width/patch_size, embedding_dim]\n","    x = x.reshape(B, -1, E)             # x shape [batch, number_of_patches, embedding_dim]\n","    return x\n","\n","class Linear_Patchification(nn.Module):\n","  \"\"\"\n","  Convolution can be replaced by linear projection\n","  If you want to implement ViT only using linear projection, you can use this class.\n","  Input shape: [batch, channel, height, width]\n","  Return: [batch, number_of_patches, embedding_dimension]\n","  \"\"\"\n","  def __init__(self, in_channels, patch_size, embedding_dim):\n","    super().__init__()\n","    \n","    # Define the linear projection layer\n","    self.patch_size = patch_size\n","    self.embedding_dim = embedding_dim\n","    self.linear = nn.Linear(in_channels * self.patch_size[0] * self.patch_size[1], embedding_dim)\n","\n","  def forward(self, x):\n","\n","    # x shape [batch, channel, height, width]\n","    B, C, H, W = x.shape  \n","    # x shape = [B, C, H/P, P, W/P, P]\n","    x = x.reshape(B, C, H//self.patch_size[0], self.patch_size[0], W//self.patch_size[1], self.patch_size[1])\n","    # x shape = [B, H/P, W/P, C, P, P]\n","    x = x.permute(0,2,4,1,3,5)\n","    # x shape = [B, (H/P)*(W/P), C, P, P]\n","    x = x.reshape(B, -1, C * self.patch_size[0] * self.patch_size[1])\n","\n","    x = self.linear(x)\n","    #####################\n","    return x"]},{"cell_type":"code","execution_count":null,"id":"0qXGdfdPVRGq","metadata":{"id":"0qXGdfdPVRGq"},"outputs":[],"source":["class MLP(nn.Module):\n","  \"\"\"\n","  Feed-forward layer\n","  Input shape: [batch, number_of_patches, embedding_dimension]\n","  Return: [batch, number_of_patches, embedding_dimension]\n","  \"\"\"\n","  def __init__(self, dim, hidden_dim, dropout = 0.):\n","    super().__init__()\n","    self.fc1 = nn.Linear(dim, hidden_dim)\n","    self.fc2 = nn.Linear(hidden_dim, dim)\n","    self.dropout = nn.Dropout(dropout)\n","    self.activation = nn.GELU()               # GELU activation function\n","\n","  def forward(self, x):\n","    x = self.fc1(x)                           # x shape [batch, number_of_patches, hidden_dim]\n","    x = self.activation(x)      \n","    x = self.dropout(x)  \n","    x = self.fc2(x)                           # x shape [batch, number_of_patches, embedding_dimension]\n","    return x"]},{"cell_type":"code","execution_count":null,"id":"W6pA1fMvVSv5","metadata":{"id":"W6pA1fMvVSv5"},"outputs":[],"source":["class Attention(nn.Module):\n","  \"\"\"\n","  Multi-head attention\n","  Input shape: [batch, number_of_patches, embedding_dimension]\n","  Return: [batch, number_of_patches, embedding_dimension]\n","  \"\"\"\n","  def __init__(self, dim, num_heads, dropout = 0.):\n","    super().__init__()\n","    self.head_dim = dim // num_heads\n","    self.dim = dim\n","    self.num_heads = num_heads\n","    self.scale = self.head_dim ** 0.5                     \n","    self.dropout = nn.Dropout(dropout)\n","    self.qkv = nn.Linear(dim, dim * 3, bias=False) \n","    self.fc = nn.Linear(dim, dim)  \n","\n","  def forward(self, x):\n","    B, N, E = x.shape\n","    # qkv shape [batch, number_of_patches, 3*embedding_dimension]\n","    qkv = self.qkv(x)  \n","    # qkv shape [batch, number_of_patches, 3, num_heads, head_dim]\n","    qkv = qkv.reshape(B, N, 3, self.num_heads, self.head_dim) \n","\n","    q = qkv[:, :, 0]         # Query shape [batch, number_of_patches, num_heads, head_dim]\n","    k = qkv[:, :, 1]         # Key shape [batch, number_of_patches, num_heads, head_dim]\n","    v = qkv[:, :, 2]         # Value shape [batch, number_of_patches, num_heads, head_dim]\n","\n","    # Compute attention scores\n","    scores = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n","    attn = torch.softmax(scores, dim=-1)\n","    attn = self.dropout(attn)\n","\n","    # attn shape [batch, num_heads, number_of_patches, number_of_patches]\n","    # v shape [batch, num_heads, number_of_patches, head_dim]\n","    weighted_values = torch.matmul(attn, v)\n","\n","    # weighted_values shape [batch, number_of_patches, num_heads, head_dim]\n","    weighted_values = weighted_values.transpose(1, 2).reshape(B, N, self.dim)\n","    x = self.fc(weighted_values)\n","    return x\n","\n","class Block(nn.Module):\n","  \"\"\"\n","  Attention block\n","  Input shape: [batch, number_of_patches, embedding_dimension]\n","  Return: [batch, number_of_patches, embedding_dimension]\n","  \"\"\"\n","  def __init__(self, dim, num_heads, mlp_dim, dropout=0.):\n","    super().__init__()\n","    self.norm1 = nn.LayerNorm(dim)\n","    self.attention = Attention(dim, num_heads, dropout=dropout)\n","    self.norm2 = nn.LayerNorm(dim)\n","    self.mlp = MLP(dim, mlp_dim, dropout=dropout)\n","\n","  def forward(self, x):\n","   \n","    norm_x = self.norm1(x)  \n","    attn_output = self.attention(norm_x)  \n","    x = x + attn_output                     #residual connection\n","    norm_x = self.norm2(x)  \n","    mlp_output = self.mlp(norm_x)  \n","    x = x + mlp_output                      #residual connection\n","    return x"]},{"cell_type":"code","execution_count":null,"id":"c4d35830","metadata":{"id":"c4d35830"},"outputs":[],"source":["class ViT(nn.Module):\n","    def __init__(self, image_shape, patch_size, num_classes, dim, num_heads, depth, mlp_dim, dropout = 0.):\n","        super().__init__()\n","        \"\"\"\n","        image_shape: [channel, height, width]\n","        patch_size: [height, width]\n","        dim: Embedding dimension\n","        num_heads: Number of heads to be used in Multi-head Attention\n","        depth: Number of attention blocks to be used\n","        mlp_dim: Hidden dimension to be used in MLP layer (=feedforward layer)\n","        \"\"\"\n","\n","        # image_ch will be 3(RGB 3 channels) for CIFAR10 dataset\n","        image_ch, image_h, image_w = image_shape \n","        patch_h, patch_w = patch_size\n","\n","        assert image_h % patch_h == 0 and image_w % patch_w == 0, 'Image height & width must be divisible by those of patch respectively.'\n","        assert dim % num_heads == 0, 'Embedding dimension should be divisible by number of heads.'\n","\n","        # e.g. [32 x 32] image & [8 x 8] patch size -> [4 x 4 = 16] patches\n","        num_patches = (image_h // patch_h) * (image_w // patch_w) \n","\n","        # Patchification using convolution.\n","        self.patchify = Patchification(image_ch, patch_size, dim)\n","\n","        # Use linear patchification if you want to use linear layer instead of convolution.\n","        # self.patchify = Linear_Patchification(image_ch, patch_size, dim)\n","\n","        # Learnable positional encoding, 1+ is for class token.\n","        self.pos_embedding = nn.Parameter(torch.randn(1, 1 + num_patches, dim)) \n","\n","        # Class token which will be prepended to each image.\n","        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n","\n","        # Initialize attention blocks\n","        self.attention_blocks = nn.ModuleList([\n","            Block(dim, num_heads, mlp_dim, dropout)\n","            for _ in range(depth)\n","        ])\n","\n","        # Classification head, maps the final vector to class dimension.\n","        self.classification_head = nn.Sequential(\n","            nn.LayerNorm(dim),\n","            nn.Linear(dim, num_classes)\n","        )\n","\n","    def forward(self, img):\n","        cls_tokens = self.cls_token.expand(img.shape[0], -1, -1)        # Shape: [batch, 1, dim]\n","        \"\"\"\n","        For classification, we need to add cls token to each image.\n","        Then, at last use cls token for classification.\n","        \"\"\"\n","        \n","        # patch shape: [batch, number_of_patches, dim]\n","        patches = self.patchify(img)  \n","        \n","        # x shape: [batch, 1 + number_of_patches, dim]\n","        x = torch.cat([cls_tokens, patches], dim=1)  \n","        x = x + self.pos_embedding \n","\n","        # Pass pathces through attention blocks\n","        for attention_block in self.attention_blocks:\n","            x = attention_block(x)  # x shape: [batch, 1 + number_of_patches, dim]\n","\n","        # Use cls token\n","        # x shape: [batch, 1, dim]\n","        x = x[:,0,:]\n","\n","        # x shape: [batch, num_classes]\n","        x = self.classification_head(x)  \n","\n","        return x"]},{"attachments":{},"cell_type":"markdown","id":"6tOh1In3x3FM","metadata":{"id":"6tOh1In3x3FM"},"source":["# ViT Image Classification"]},{"cell_type":"code","execution_count":null,"id":"VbHlUuyTdhK2","metadata":{"id":"VbHlUuyTdhK2"},"outputs":[],"source":["DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","print(\"Using PyTorch version: {}, Device: {}\".format(torch.__version__, DEVICE))"]},{"cell_type":"code","execution_count":null,"id":"773eb49a","metadata":{},"outputs":[],"source":["def train(model, train_loader, optimizer, criterion, DEVICE):\n","    model.train()\n","    tqdm_bar = tqdm(train_loader)\n","    for batch_idx, (image, label) in enumerate(tqdm_bar):\n","        image = image.to(DEVICE)\n","        label = label.to(DEVICE)\n","        optimizer.zero_grad()\n","        output = model(image)\n","        loss = criterion(output, label)\n","        loss.backward()\n","        optimizer.step()\n","        tqdm_bar.set_description(\"Epoch {} - train loss: {:.6f}\".format(epoch, loss.item()))\n","\n","\n","def evaluate(model, test_loader, criterion, DEVICE):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","\n","    with torch.no_grad():\n","        for image, label in tqdm(test_loader):\n","            image = image.to(DEVICE)\n","            label = label.to(DEVICE)\n","            output = model(image)\n","            test_loss += criterion(output, label).item()\n","            prediction = output.max(1, keepdim=True)[1]\n","            correct += prediction.eq(label.view_as(prediction)).sum().item()\n","    \n","    test_loss /= len(test_loader.dataset)\n","    test_accuracy = 100. * correct / len(test_loader.dataset)\n","    return test_loss, test_accuracy"]},{"cell_type":"code","execution_count":null,"id":"mOGBSL3bz0jX","metadata":{"id":"mOGBSL3bz0jX"},"outputs":[],"source":["BATCH_SIZE = 100\n","\n","train_transform = transforms.Compose([\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","test_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","# Prepare Dataset & DataLoader\n","trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n","\n","testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)"]},{"cell_type":"code","execution_count":null,"id":"bzkZ228b8rN5","metadata":{"id":"bzkZ228b8rN5"},"outputs":[],"source":["# Hyperparameters\n","EPOCHS = 10\n","patch_size = (4,4)\n","dim = 128\n","depth = 8\n","num_heads = 8\n","mlp_dim = 256\n","dropout = 0.\n","learning_rate = 0.001\n","\n","model = ViT(image_shape = (3,32,32), patch_size = patch_size, num_classes = 10, dim = dim, num_heads = num_heads, depth = depth, mlp_dim = mlp_dim, dropout=dropout).to(DEVICE)\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","criterion = nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":null,"id":"sjeFjn0qrPRK","metadata":{"id":"sjeFjn0qrPRK"},"outputs":[],"source":["# Train \n","\n","for epoch in range(1, EPOCHS + 1):\n","    train(model, trainloader, optimizer, criterion, DEVICE)\n","    test_loss, test_accuracy = evaluate(model, testloader, criterion, DEVICE)\n","    print(\"\\n[EPOCH: {}], \\tModel: ViT, \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n","        epoch, test_loss, test_accuracy))"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["mDOdbok4zVE4","giBbJLW-zhS-","2f33b210","2eeea83e","6tOh1In3x3FM"],"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}
